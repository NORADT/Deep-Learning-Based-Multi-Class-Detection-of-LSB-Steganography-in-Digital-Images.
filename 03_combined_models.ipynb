{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout,\n",
    "    Reshape, LSTM, GRU, Bidirectional, Input,\n",
    "    Concatenate, Multiply\n",
    ")\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from transformers import ViTImageProcessor, TFViTForImageClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Downloading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    path = kagglehub.dataset_download(\"marcozuppelli/stegoimagesdataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    source_path = path\n",
    "    destination_path = '/content/stegoimagesdataset'\n",
    "\n",
    "    if os.path.exists(destination_path):\n",
    "        shutil.rmtree(destination_path)\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    for item in os.listdir(source_path):\n",
    "        s = os.path.join(source_path, item)\n",
    "        d = os.path.join(destination_path, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "    print(f\"Data copied to {destination_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not download from Kaggle Hub. Error: {e}\")\n",
    "    destination_path = '/content/stegoimagesdataset'\n",
    "    if not os.path.exists(destination_path):\n",
    "        raise FileNotFoundError(\"Dataset not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segregate_data_by_payload(original_data_path, new_base_path):\n",
    "    print(\"\\nSegregating data by payload type...\")\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        stego_path = os.path.join(original_data_path, split, split, 'stego')\n",
    "        clean_path = os.path.join(original_data_path, split, split, 'clean')\n",
    "        new_split_path = os.path.join(new_base_path, split)\n",
    "\n",
    "        if os.path.exists(stego_path):\n",
    "            for img_name in os.listdir(stego_path):\n",
    "                try:\n",
    "                    payload_class = img_name.split(\"_\")[2]\n",
    "                    class_dir = os.path.join(new_split_path, payload_class)\n",
    "                    os.makedirs(class_dir, exist_ok=True)\n",
    "                    shutil.copy(os.path.join(stego_path, img_name), class_dir)\n",
    "                except IndexError:\n",
    "                    print(f\"Skipping file with unexpected name format: {img_name}\")\n",
    "\n",
    "        if os.path.exists(clean_path):\n",
    "            new_clean_path = os.path.join(new_split_path, 'clean')\n",
    "            shutil.copytree(clean_path, new_clean_path, dirs_exist_ok=True)\n",
    "    print(\"Data segregation complete.\")\n",
    "\n",
    "new_data_path = \"new_data\"\n",
    "if os.path.exists(new_data_path):\n",
    "    shutil.rmtree(new_data_path)\n",
    "segregate_data_by_payload(destination_path, new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_resplit(source_base_path, final_base_path, train_ratio=0.8, test_ratio=0.1):\n",
    "    print(\"\\nCombining and re-splitting data...\")\n",
    "    all_files_by_class = {}\n",
    "    for class_name in os.listdir(os.path.join(source_base_path, 'train')):\n",
    "        all_files_by_class[class_name] = []\n",
    "        for split in ['train', 'test', 'val']:\n",
    "            class_folder = os.path.join(source_base_path, split, class_name)\n",
    "            if os.path.exists(class_folder):\n",
    "                all_files_by_class[class_name].extend([os.path.join(class_folder, f) for f in os.listdir(class_folder)])\n",
    "\n",
    "    if os.path.exists(final_base_path):\n",
    "        shutil.rmtree(final_base_path)\n",
    "\n",
    "    for class_name, files in all_files_by_class.items():\n",
    "        train_files, temp_files = train_test_split(files, train_size=train_ratio, random_state=42)\n",
    "        val_ratio = 1 - (test_ratio / (1 - train_ratio))\n",
    "        val_files, test_files = train_test_split(temp_files, train_size=val_ratio, random_state=42)\n",
    "\n",
    "        for split_name, file_list in [('train', train_files), ('val', val_files), ('test', test_files)]:\n",
    "            dest_dir = os.path.join(final_base_path, split_name, class_name)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            for f in file_list:\n",
    "                shutil.copy(f, dest_dir)\n",
    "    print(\"Data re-splitting complete.\")\n",
    "\n",
    "final_data_path = \"final_payload_data\"\n",
    "combine_and_resplit(new_data_path, final_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE_GENERAL = 512\n",
    "IMG_SIZE_VIT = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"\\nLoading data for general models (CNN, RNNs) with image size {IMG_SIZE_GENERAL}x{IMG_SIZE_GENERAL}...\")\n",
    "train_dataset_gen = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(final_data_path, \"train\"),\n",
    "    labels='inferred', label_mode='int',\n",
    "    image_size=(IMG_SIZE_GENERAL, IMG_SIZE_GENERAL),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, seed=123\n",
    ")\n",
    "val_dataset_gen = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(final_data_path, \"val\"),\n",
    "    labels='inferred', label_mode='int',\n",
    "    image_size=(IMG_SIZE_GENERAL, IMG_SIZE_GENERAL),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, seed=123\n",
    ")\n",
    "test_dataset_gen = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(final_data_path, \"test\"),\n",
    "    labels='inferred', label_mode='int',\n",
    "    image_size=(IMG_SIZE_GENERAL, IMG_SIZE_GENERAL),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading data for ViT model with image size {IMG_SIZE_VIT}x{IMG_SIZE_VIT}...\")\n",
    "train_dataset_vit = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(final_data_path, \"train\"),\n",
    "    labels='inferred', label_mode='int',\n",
    "    image_size=(IMG_SIZE_VIT, IMG_SIZE_VIT),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, seed=123\n",
    ")\n",
    "val_dataset_vit = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(final_data_path, \"val\"),\n",
    "    labels='inferred', label_mode='int',\n",
    "    image_size=(IMG_SIZE_VIT, IMG_SIZE_VIT),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, seed=123\n",
    ")\n",
    "test_dataset_vit = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(final_data_path, \"test\"),\n",
    "    labels='inferred', label_mode='int',\n",
    "    image_size=(IMG_SIZE_VIT, IMG_SIZE_VIT),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_dataset_gen.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f\"Found {num_classes} classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "def prepare(ds, augment=False):\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds_gen = prepare(train_dataset_gen, augment=True)\n",
    "val_ds_gen = prepare(val_dataset_gen)\n",
    "test_ds_gen = prepare(test_dataset_gen)\n",
    "\n",
    "train_ds_vit = prepare(train_dataset_vit, augment=True)\n",
    "val_ds_vit = prepare(val_dataset_vit)\n",
    "test_ds_vit = prepare(test_dataset_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Individual Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255, input_shape=input_shape),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu', name='feature_layer'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=\"CNN_Model\")\n",
    "    return model\n",
    "\n",
    "def build_rnn_model(input_shape, num_classes, rnn_type='lstm'):\n",
    "    features_per_timestep = input_shape[1] * input_shape[2]\n",
    "    timesteps = input_shape[0]\n",
    "    RNN_LAYER = LSTM if rnn_type.lower() == 'lstm' else GRU\n",
    "\n",
    "    model = Sequential([\n",
    "        Reshape((timesteps, features_per_timestep), input_shape=input_shape),\n",
    "        Bidirectional(RNN_LAYER(128, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(RNN_LAYER(64, name='feature_layer')),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=f\"{rnn_type.upper()}_Model\")\n",
    "    return model\n",
    "\n",
    "def build_vit_model(num_classes):\n",
    "    model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "    model = TFViTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_gen = (IMG_SIZE_GENERAL, IMG_SIZE_GENERAL, 3)\n",
    "cnn_model = build_cnn_model(input_shape_gen, num_classes)\n",
    "lstm_model = build_rnn_model(input_shape_gen, num_classes, rnn_type='lstm')\n",
    "gru_model = build_rnn_model(input_shape_gen, num_classes, rnn_type='gru')\n",
    "vit_model = build_vit_model(num_classes)\n",
    "\n",
    "print(\"--- CNN Model Summary ---\")\n",
    "cnn_model.summary()\n",
    "print(\"\\n--- LSTM Model Summary ---\")\n",
    "lstm_model.summary()\n",
    "print(\"\\n--- GRU Model Summary ---\")\n",
    "gru_model.summary()\n",
    "print(\"\\n--- ViT Model (structure) ---\")\n",
    "print(f\"ViT model loaded: {vit_model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"CNN\": cnn_model,\n",
    "    \"LSTM\": lstm_model,\n",
    "    \"GRU\": gru_model,\n",
    "    \"ViT\": vit_model\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "trained_models = {}\n",
    "\n",
    "vit_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} Model ---\")\n",
    "    \n",
    "    if name == 'ViT':\n",
    "        def vit_prepare_fn(image, label):\n",
    "            return {'pixel_values': image}, label\n",
    "\n",
    "        train_ds_vit_prepared = train_ds_vit.map(vit_prepare_fn)\n",
    "        val_ds_vit_prepared = val_ds_vit.map(vit_prepare_fn)\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "                      loss=vit_loss,\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds_vit_prepared,\n",
    "            validation_data=val_ds_vit_prepared,\n",
    "            epochs=5,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds_gen,\n",
    "            validation_data=val_ds_gen,\n",
    "            epochs=10,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "    histories[name] = history\n",
    "    trained_models[name] = model\n",
    "    print(f\"--- {name} Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Individual Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred_proba, num_classes, class_names, model_name):\n",
    "    y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = np.trapz(tpr[i], fpr[i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "    roc_auc[\"micro\"] = np.trapz(tpr[\"micro\"], fpr[\"micro\"])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                 label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Multi-class ROC Curve for {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_ds, model_name, is_vit=False):\n",
    "    print(f\"\\n--- Evaluating {model_name} Model ---\")\n",
    "    test_ds_eval = test_ds\n",
    "    if is_vit:\n",
    "        test_ds_eval = test_ds.map(lambda x, y: ({'pixel_values': x}, y))\n",
    "\n",
    "    y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    predictions = model.predict(test_ds_eval)\n",
    "    \n",
    "    y_pred_proba = predictions\n",
    "    if is_vit:\n",
    "        y_pred_proba = tf.nn.softmax(predictions.logits, axis=-1).numpy()\n",
    "        \n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "        print(f\"Weighted Average ROC-AUC Score: {roc_auc:.4f}\")\n",
    "        plot_roc_curve(y_true, y_pred_proba, num_classes, class_names, model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate ROC-AUC score: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in trained_models.items():\n",
    "    is_vit_model = (name == \"ViT\")\n",
    "    current_test_ds = test_ds_vit if is_vit_model else test_ds_gen\n",
    "    evaluate_model(model, current_test_ds, name, is_vit=is_vit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Combination Technique 1: Feature Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Combination Technique 1: Feature Fusion ---\")\n",
    "\n",
    "feature_extractors = {}\n",
    "feature_extractors['CNN'] = Model(\n",
    "    inputs=cnn_model.input,\n",
    "    outputs=cnn_model.get_layer('feature_layer').output,\n",
    "    name=\"CNN_Feature_Extractor\"\n",
    ")\n",
    "feature_extractors['LSTM'] = Model(\n",
    "    inputs=lstm_model.input,\n",
    "    outputs=lstm_model.get_layer('feature_layer').output,\n",
    "    name=\"LSTM_Feature_Extractor\"\n",
    ")\n",
    "feature_extractors['GRU'] = Model(\n",
    "    inputs=gru_model.input,\n",
    "    outputs=gru_model.get_layer('feature_layer').output,\n",
    "    name=\"GRU_Feature_Extractor\"\n",
    ")\n",
    "feature_extractors['ViT'] = Model(\n",
    "    inputs=trained_models['ViT'].input,\n",
    "    outputs=trained_models['ViT'].vit.pooler.output,\n",
    "    name=\"ViT_Feature_Extractor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_model(model, dataset, is_vit=False):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    dataset_to_use = dataset\n",
    "    if is_vit:\n",
    "        dataset_to_use = dataset.map(lambda x, y: ({'pixel_values': x}, y))\n",
    "\n",
    "    for data_batch, labels_batch in dataset_to_use:\n",
    "        feats = model.predict(data_batch)\n",
    "        features_list.append(feats)\n",
    "        labels_list.append(labels_batch.numpy())\n",
    "    \n",
    "    return np.concatenate(features_list), np.concatenate(labels_list)\n",
    "\n",
    "train_features, val_features, test_features = {}, {}, {}\n",
    "train_labels, val_labels, test_labels = None, None, None\n",
    "\n",
    "for name, extractor in feature_extractors.items():\n",
    "    print(f\"Extracting features using {name}...\")\n",
    "    is_vit = (name == 'ViT')\n",
    "    \n",
    "    current_train_ds = train_ds_vit if is_vit else train_ds_gen\n",
    "    current_val_ds = val_ds_vit if is_vit else val_ds_gen\n",
    "    current_test_ds = test_ds_vit if is_vit else test_ds_gen\n",
    "\n",
    "    train_features[name], temp_train_labels = extract_features_from_model(extractor, current_train_ds, is_vit)\n",
    "    val_features[name], temp_val_labels = extract_features_from_model(extractor, current_val_ds, is_vit)\n",
    "    test_features[name], temp_test_labels = extract_features_from_model(extractor, current_test_ds, is_vit)\n",
    "\n",
    "    if train_labels is None:\n",
    "        train_labels = temp_train_labels\n",
    "        val_labels = temp_val_labels\n",
    "        test_labels = temp_test_labels\n",
    "\n",
    "print(\"\\nFeatures extracted for all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Concatenation Fusion ---\")\n",
    "\n",
    "def build_and_evaluate_concat_fusion(model_names, train_feats, val_feats, test_feats):\n",
    "    print(f\"\\nFusing models: {', '.join(model_names)}\")\n",
    "    \n",
    "    train_input = [train_features[name] for name in model_names]\n",
    "    val_input = [val_features[name] for name in model_names]\n",
    "    test_input = [test_features[name] for name in model_names]\n",
    "\n",
    "    inputs = [Input(shape=train_features[name].shape[1:]) for name in model_names]\n",
    "    concatenated = Concatenate()(inputs)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(concatenated)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    fusion_model = Model(inputs=inputs, outputs=output)\n",
    "    fusion_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    fusion_model.fit(train_input, train_labels,\n",
    "                     validation_data=(val_input, val_labels),\n",
    "                     epochs=20, batch_size=BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    y_pred_proba = fusion_model.predict(test_input)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_labels, y_pred, target_names=class_names))\n",
    "    roc_auc = roc_auc_score(test_labels, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "    print(f\"Weighted Average ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "build_and_evaluate_concat_fusion(['CNN', 'ViT'], train_features, val_features, test_features)\n",
    "build_and_evaluate_concat_fusion(['LSTM', 'ViT'], train_features, val_features, test_features)\n",
    "build_and_evaluate_concat_fusion(['CNN', 'LSTM', 'ViT'], train_features, val_features, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Element-wise Product Fusion ---\")\n",
    "PROJECTION_DIM = 128\n",
    "\n",
    "def build_and_evaluate_product_fusion(model_names, train_feats, val_feats, test_feats):\n",
    "    print(f\"\\nFusing models (Product): {', '.join(model_names)}\")\n",
    "\n",
    "    inputs = [Input(shape=train_features[name].shape[1:]) for name in model_names]\n",
    "    projected_layers = [Dense(PROJECTION_DIM, activation='relu')(inp) for inp in inputs]\n",
    "    \n",
    "    multiplied = Multiply()(projected_layers)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(multiplied)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    fusion_model = Model(inputs=inputs, outputs=output)\n",
    "    fusion_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    train_input = [train_features[name] for name in model_names]\n",
    "    val_input = [val_features[name] for name in model_names]\n",
    "    test_input = [test_features[name] for name in model_names]\n",
    "\n",
    "    fusion_model.fit(train_input, train_labels,\n",
    "                     validation_data=(val_input, val_labels),\n",
    "                     epochs=20, batch_size=BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    y_pred_proba = fusion_model.predict(test_input)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_labels, y_pred, target_names=class_names))\n",
    "    roc_auc = roc_auc_score(test_labels, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "    print(f\"Weighted Average ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "build_and_evaluate_product_fusion(['CNN', 'ViT'], train_features, val_features, test_features)\n",
    "build_and_evaluate_product_fusion(['LSTM', 'ViT'], train_features, val_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Combination Technique 2: Ensemble Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Combination Technique 2: Ensemble Voting ---\")\n",
    "\n",
    "all_pred_probas = {}\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"Getting predictions from {name}...\")\n",
    "    if name == 'ViT':\n",
    "        test_ds_vit_prepared = test_ds_vit.map(lambda x, y: ({'pixel_values': x}, y))\n",
    "        predictions = model.predict(test_ds_vit_prepared)\n",
    "        all_pred_probas[name] = tf.nn.softmax(predictions.logits, axis=-1).numpy()\n",
    "    else:\n",
    "        all_pred_probas[name] = model.predict(test_ds_gen)\n",
    "\n",
    "y_true_ensemble = np.concatenate([y for x, y in test_ds_gen], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Hard Voting (Majority Voting) ---\")\n",
    "all_preds = [np.argmax(p, axis=1) for p in all_pred_probas.values()]\n",
    "stacked_preds = np.stack(all_preds, axis=1)\n",
    "\n",
    "hard_vote_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=stacked_preds)\n",
    "\n",
    "print(\"Classification Report (Hard Voting):\")\n",
    "print(classification_report(y_true_ensemble, hard_vote_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Soft Voting (Averaging Probabilities) ---\")\n",
    "avg_proba = np.mean(list(all_pred_probas.values()), axis=0)\n",
    "soft_vote_preds = np.argmax(avg_proba, axis=1)\n",
    "\n",
    "print(\"Classification Report (Soft Voting):\")\n",
    "print(classification_report(y_true_ensemble, soft_vote_preds, target_names=class_names))\n",
    "\n",
    "roc_auc_soft = roc_auc_score(y_true_ensemble, avg_proba, multi_class='ovr', average='weighted')\n",
    "print(f\"Weighted Average ROC-AUC Score (Soft Voting): {roc_auc_soft:.4f}\")\n",
    "\n",
    "plot_roc_curve(y_true_ensemble, avg_proba, num_classes, class_names, \"Soft Voting Ensemble\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
